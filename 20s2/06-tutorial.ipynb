{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 6 Tutorial - Web Scraping\n",
    "\n",
    "Sometimes, it may seem difficult to find the 'right' sort of data for your business concern. In today's tutorial, we'll be looking at one particular way of mitigating the challenges associated with finding the 'right' data: web scraping. This is a powerful tool for improving your analytics process, by extending your data acquisition to virtually anything on the world wide web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restocking A Franchise Of Restaurants\n",
    "\n",
    "### [1] Question - Getting Product Information From Kmart\n",
    "\n",
    "Let us consider the business concern of today's tutorial:\n",
    "\n",
    "> **CONCERN:** The Sky Village Group is well-recognised for being a company that successfully manages over 200 restaurants worldwide. However, maintaining these restaurants is no easy task, with many overdue for the replacement of cutlery, crockery, and utensils. After many discussions, the management team has agreed that the company will contract popular lifestyle outlet Kmart for the retrieval of the many required items, however knowing that Kmart offers a range of brands for each product means that an analytics process should inform the decisions made about the purchases.\n",
    "\n",
    "The products being sought are given as follows:\n",
    "\n",
    "- Salt & Pepper Shakers\n",
    "- Plates\n",
    "- Spoons\n",
    "- Forks\n",
    "- Containers\n",
    "- Jugs\n",
    "\n",
    "In the decision making process, the priorities will be __cost efficiency__, and following a __metallic__ color scheme. Can you help the Sky Village Group?\n",
    "\n",
    ">**QUESTION:** Where might we retrieve the information for this particular business concern?\n",
    "\n",
    ">>**ANSWER:** ???\n",
    "\n",
    ">**QUESTION:** Can you explain your answer to the previous question?\n",
    "\n",
    ">>**ANSWER:** ???\n",
    "\n",
    "As a foreword for the proceeding data analytics process, we mention that scraping is perhaps the most __volatile__ form of data acquisition.\n",
    "\n",
    ">**QUESTION:** How might this affect your analysis?\n",
    "\n",
    ">>**ANSWER:** ???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2] Data - A Gentle Introduction To Web Scraping\n",
    "\n",
    "As already indicated by the business concern, the data we need is in the catalogue of Kmart's products. You could very easily visit https://www.kmart.com.au/ to search for the desired products, and could make comparisons between them to inform your analysis, but this would be tedious, and would take up much of the necessary time to work on your insights instead.\n",
    "\n",
    "As an alternative, we are going to consider using a simple method of scraping (via the `requests` package) to retrieve the information we need.\n",
    "\n",
    "__NOTE:__ For this task, you will need the Chrome Web Browser (Mozilla Firefox is also acceptable). \n",
    "\n",
    "__Step 1:__ Begin by opening up your web browser, and navigating to the Kmart web page.\n",
    "\n",
    "<img src=\"graphics/6_1.png\" style=\"margin-left: 50px; width: 60%;\">\n",
    "\n",
    "__Step 2:__ Next, right click the page, and click 'Inspect' to open up the Chrome 'Developer Tools' window. \n",
    "\n",
    "<img src=\"graphics/6_2.png\" style=\"margin-left: 50px; width: 60%;\">\n",
    "\n",
    "__Step 3:__ Navigate to the 'Network' tab, click the 'Record' button, wipe the log, and then check the 'Preserve Log' option. This will make the web browser record incoming HTTP traffic.  \n",
    "\n",
    "<img src=\"graphics/6_3.png\" style=\"margin-left: 50px; width: 60%;\">\n",
    "\n",
    "__Step 4:__ Then, with the developer tools still open, navigate back to the web page, and input a search term of your choosing e.g. 'cups'.\n",
    "\n",
    "<img src=\"graphics/6_4.png\" style=\"margin-left: 50px; width: 60%;\">\n",
    "\n",
    "__Step 5:__ When the page loads the results for your search term, navigate back to your developer tools, and within the Network tab, you'll notice that some new content now populates the list.\n",
    "\n",
    "<img src=\"graphics/6_5.png\" style=\"margin-left: 50px; width: 60%;\">\n",
    "\n",
    "__Step 6:__ Now click __CTRL+F / Command+F__ to open the search panel on the left side of the network tab. In the input field, type in your original search term (in the example case, it was 'cups').\n",
    "\n",
    "<img src=\"graphics/6_6.png\" style=\"margin-left: 50px; width: 60%;\">\n",
    "\n",
    "__Step 7:__ In the results that appear, note the names of the requests that contain information about your search term. Sift through each, and select the option whose HTML seems most related to the search term (this is a bit of a rigorous step, so don't rush it too quickly).\n",
    "\n",
    "<img src=\"graphics/6_7.png\" style=\"margin-left: 50px; width: 60%;\">\n",
    "\n",
    "__Step 8:__ With the noted request, filter it in the top menu of the Network tab.\n",
    "\n",
    "<img src=\"graphics/6_8.png\" style=\"margin-left: 50px; width: 60%;\">\n",
    "\n",
    "__Step 9:__ Then right click the request, and click the 'Copy as cURL' option that appears.\n",
    "\n",
    "<img src=\"graphics/6_9.png\" style=\"margin-left: 50px; width: 60%;\">\n",
    "\n",
    "__Step 10:__ Now navigate to https://curl.trillworks.com/#python and paste your result in the input section on the left side of the window.\n",
    "\n",
    "<img src=\"graphics/6_10.png\" style=\"margin-left: 50px; width: 60%;\">\n",
    "\n",
    "__Step 11:__ Conveniently, the web page will punch out the required code on the right side of the window. Now copy the content, and place it in the code block that follows.\n",
    "\n",
    "<img src=\"graphics/6_11.png\" style=\"margin-left: 50px; width: 60%;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ##################################\n",
    "# \n",
    "# PUT THE CODE THAT IS RETURNED FROM THE PYTHON INTERPRETATION OF THE CURL COMMAND HERE\n",
    "#\n",
    "# ##################################\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the code that you pasted above, to interpret how it is obtaining the required data from the web page.\n",
    "\n",
    ">**QUESTION:** What is this code's purpose, and what is returned to the `response` variable?\n",
    "\n",
    ">>**ANSWER:** ???\n",
    "\n",
    ">**QUESTION:** Can you guess what the purposes are of the `cookies`, `headers`, `params`, and `data` variables?\n",
    "\n",
    ">>**ANSWER:** ???\n",
    "\n",
    ">**QUESTION:** Where does the original search term of 'cups' play into this piece of code?\n",
    "\n",
    ">>**ANSWER:** ???\n",
    "\n",
    "What is interesting about the code above is that it doesn't just work for `cups`, but can be adapted to any search term that you supply. Let's consider augmenting the code into a function that returns the HTML for any search page concerning some term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_kmart_search_page(term):\n",
    "    \n",
    "    # ONCE AGAIN, YOUR CODE GOES HERE\n",
    "    \n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try returning the HTML of another item, to see whether it can handle any arbitrary term:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_text = get_kmart_search_page(???)\n",
    "response_text[:2000] # We'll only get the first chunk of the file (IT'S REALLY BIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3] Analysis - Extracting Product Details\n",
    "\n",
    "The HTML returned from the function we defined is quite difficult to interpret. To overcome this, we have to become proficient at isolating important pieces of text. Consider our example search term ('cups'); we could visit the search page to see the results that were returned, and then cross-check it with the HTML version of the page to find out where the content is appearing. One such example can be seen in the picture below, where we view a search result on the web page, and can then find the corresponding text in the code:\n",
    "\n",
    "<img src=\"graphics/6_12.png\" style=\"margin-left: 50px; width: 60%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**QUESTION:** Can you find the titles of your items in the returned HTML, as well as the corresponding prices?\n",
    "\n",
    ">>**ANSWER:** ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've found where our product details are located, we need to devise a method for extracting the necessary information. To do this, we are going to use a popular Regex solution that can isolate any text between certain character patterns:\n",
    "\n",
    "<img src=\"graphics/6_13.png\" style=\"margin-left: 50px; width: 60%;\">\n",
    "\n",
    "When written into a Regex `findall` function, it will get all text (the green part) in the returned HTML that start with the `opening anchor` and close with the `closing anchor`. \n",
    "\n",
    "Complete the Regex terms in the code block below to isolate the content for a given search term's HTML:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### You will need to use the following python commands:\n",
    "\n",
    "```python\n",
    "    import\n",
    "    re.findall()\n",
    "    pd.Dataframe()\n",
    "    min()\n",
    "```\n",
    "\n",
    "#### And you will need to import the following packages:\n",
    "\n",
    "```python\n",
    "    re\n",
    "    pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??? ???\n",
    "??? ??? as pd\n",
    "\n",
    "def isolate_search_details(HTML):\n",
    "    results_list = []\n",
    "    # Title of product\n",
    "    product_titles = ???.???(r\"(?<=<p class=\\\"title\\\">).*?(?=<\\/p>)\",HTML)\n",
    "    # Price of product\n",
    "    product_prices = [float(x) for x in ???.???(r\"(?<=itemprop=\\\"price\\\" content=\\\").*?(?=\\\">)\",HTML)]\n",
    "    # Image of product\n",
    "    product_images = ???.???(r\"(?<=\\\" border=\\\"0\\\"  src=\\\").*?(?=\\\"\\/>)\",HTML)\n",
    "    min_list_size = ???(len(product_titles), len(product_prices), len(product_images))\n",
    "    return ???.???({'Titles':product_titles[:min_list_size], 'Prices':product_prices[:min_list_size], 'Images':product_images[:min_list_size]}) \n",
    "\n",
    "isolate_search_details(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll attempt to create `result` variables, to store all the results that emerge from the necessary search terms of today's tutorial. See if you can finish off the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_salt_and_pepper_shakers = isolate_search_details(get_kmart_search_page('salt and pepper shakers'))\n",
    "??? = isolate_search_details(get_kmart_search_page(???))\n",
    "??? = isolate_search_details(get_kmart_search_page(???))\n",
    "??? = isolate_search_details(get_kmart_search_page(???))\n",
    "??? = isolate_search_details(get_kmart_search_page(???))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4] Visualisation - Pricing Bar Graphs & HTML Image Retrieval\n",
    "\n",
    "Now that we have processed our data into meaningful information, we can form an understanding about it through careful visualisation. For this task, let's consider writing a function to plot any of the dataframes that were returned from our `result` variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### You will need to use the following python commands:\n",
    "\n",
    "```python\n",
    "    .sort_values()\n",
    "    .bar()\n",
    "    .loc[]\n",
    "    .xticks()\n",
    "    .show()\n",
    "```\n",
    "\n",
    "#### And you will need to import the following packages:\n",
    "\n",
    "```python\n",
    "    matplotlib.pyplot\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ???.??? as plt\n",
    "\n",
    "def plot_result_prices(df_raw):\n",
    "    df = df_raw.???('Prices')\n",
    "    plt.???(df.???[:,'Titles'],df.???[:,'Prices'])\n",
    "    plt.???(df.???[:,'Titles'],rotation=90)\n",
    "    plt.???()\n",
    "\n",
    "plot_result_prices(results_salt_and_pepper_shakers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function will help us visualise the pricing for any item in our search results, however recall that one of the priorities of the business concern was also being able to stick to a metallic color scheme. For this purpose, we'll write yet another visualisation function to allow us to see what our search results look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You will need to use the following python commands:\n",
    "\n",
    "```python\n",
    "    .iterrows()\n",
    "```\n",
    "\n",
    "#### And you will need to import the following packages:\n",
    "\n",
    "```python\n",
    "    IPython.core.display\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ???.???.??? import display, HTML\n",
    "\n",
    "def display_result_images(df):\n",
    "    for row in df.???():\n",
    "        display(HTML('<h4>%s<h4><img src=\"%s\" style=\"float:left;width:100px\">' \n",
    "                     % (row[1]['Titles'], \"https://www.kmart.com.au/\"+row[1]['Images'])))\n",
    "\n",
    "display_result_images(results_spoons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5] Insights - Cost Efficiency vs. Metallic Color Scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we now have all the required ingredients to complete our data analytics process, it is upto you to determine what products would be most suitable for the Sky Village Group, through argument of either cost efficiency or adhering to the metallic color scheme.\n",
    "\n",
    "Some important questions to think about while coming up with your recommendation are given as follows:\n",
    "\n",
    ">**QUESTION:** What sort of cost improvements could be seen if we relinquished the need for the metallic color scheme? Is this is a substantial drawback to consider?\n",
    "\n",
    ">>**ANSWER:** ???\n",
    "\n",
    ">**QUESTION:** Is the web scraping method always convenient to our needs? If not, how so? Can you think of a situation where the effort required to implement it might outweigh the benefits? \n",
    "\n",
    ">>**ANSWER:** ???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
