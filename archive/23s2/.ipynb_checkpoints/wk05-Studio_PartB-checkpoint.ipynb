{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>IAB303</b> - Data Analytics for Business Insight</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Studio :: Digital ethics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> ***It is possible for analysis to be acurate without being fair.***\n",
    "\n",
    "Consider a scenario where we are analysing data from a survey completed by employees from a company. The employees were asked to rank how fair they believe their workplace to be on a scale as follows:\n",
    "\n",
    "1. Very unfair\n",
    "2. Unfair\n",
    "3. Mostly fair\n",
    "4. Fair\n",
    "5. Very fair\n",
    "\n",
    "Our analysis will give feedback to the company management on how well the company is doing in being fair to it's workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "file = '../data/week-7/fair-workplace-survey.csv'\n",
    "df = pandas.read_csv(file, index_col='id')\n",
    "ratings = df['FairWorkPlace']\n",
    "len(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There are 20 responses to the survey. Let's see what the average rating is to give us an idea of the overall fairness..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "ratings.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So this is looking good. The average rating is between 'Mostly fair' and 'Fair'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The average problem\n",
    "\n",
    "Consider what the average would be if we had 10 'Very unfair' (1) responses, and 10 'Very fair' (5) responses.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{(10\\times 1) + (10\\times 5)}{20} = 3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The result is 'Mostly fair' even though half of the people said 'Very unfair' and the other half 'Very fair'. Do you think that this is a *fair* interpretation?\n",
    "\n",
    "However, this type of bipolar distribution is unusual. Let's check the shape of our actual data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "hist = ratings.hist(bins=5)\n",
    "hist.set(title='Number of ratings',xlabel='rating', ylabel='count')\n",
    "hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Even better. It looks like the highest rating was 4 which is very good news for the company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Digging deeper\n",
    "\n",
    "However, if we consider the human factors behind the data, would the results be so positive?\n",
    "\n",
    "Although the survey was anonymous, we have 2 other types of information available: the gender and role of the respondants. Our respondants indicated whether they are Male or Female and if they are a Worker or a Supervisor.\n",
    "\n",
    "What's the average rating for a female worker?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "femaleWorker = df.loc[(df['Gender'] == 'F') & (df['Role'] == 'W')]\n",
    "femaleWorker['FairWorkPlace'].mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How does this compare with the average that we calculated above?\n",
    "\n",
    "Let's get a better idea by segmenting the data and finding the averages of each segment..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "female = (df['Gender'] == 'F')\n",
    "male = (df['Gender'] == 'M')\n",
    "worker = (df['Role'] == 'W')\n",
    "supervisor = (df['Role'] == 'S')\n",
    "\n",
    "def averageRating(type1,type2):\n",
    "    return df.loc[type1 & type2]['FairWorkPlace'].mean(0)\n",
    "\n",
    "segments = {}\n",
    "segments['FemaleWorker'] = averageRating(female,worker)\n",
    "segments['FemaleSupervisor'] = averageRating(female,supervisor)\n",
    "segments['MaleWorker'] = averageRating(male,worker)\n",
    "segments['MaleSupervisor'] = averageRating(male,supervisor)\n",
    "segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This tells a different story than our first histogram. Let's visualise this data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "names = list(segments.keys())\n",
    "values = list(segments.values())\n",
    "\n",
    "plt.bar(range(len(segments)),values,tick_label=names)\n",
    "plt.xticks(rotation=20)\n",
    "plt.suptitle('Workplace Fairness Ratings (by Segment)', fontsize=14)\n",
    "plt.xlabel('Gender-Role', fontsize=13)\n",
    "plt.ylabel('Average Rating', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What can we learn?\n",
    "\n",
    "* What is the story of the segment visualisation?\n",
    "* How does this different from the original story?\n",
    "* Was the first analysis wrong?\n",
    "* If we didn't dig deeper, how fair would our analysis be?\n",
    "* What is the difference between accurate analysis and fair analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Anscombe's Quartet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Load the example dataset for Anscombe's quartet\n",
    "df = sns.load_dataset(\"anscombe\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "d1 = df[df['dataset']=='I']\n",
    "d1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "d2 = df[df['dataset']=='II']\n",
    "d2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "d3 = df[df['dataset']=='III']\n",
    "d3.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "d4 = df[df['dataset']=='IV']\n",
    "d4.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Show the results of a linear regression within each dataset\n",
    "sns.set(style=\"ticks\")\n",
    "sns.lmplot(x=\"x\", y=\"y\", col=\"dataset\", hue=\"dataset\", data=df,\n",
    "           col_wrap=2, ci=None, palette=\"muted\", height=4,\n",
    "           scatter_kws={\"s\": 50, \"alpha\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### A more complex example\n",
    "\n",
    "Read through the following example which explains how similar biases can occur when working with more complicated machine learning algorithms:\n",
    "\n",
    "[Google Developers - Text Embedding Models Contain Bias. Here's Why That Matters.](https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html?m=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "\n",
    "# Trust and other human factors in business data analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Without cues for depth...\n",
    "\n",
    "[Strata 2013: Kate Crawford, \"Algorithmic Illusions: Hidden Biases of Big Data\"](https://youtu.be/irP5RCdpilc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MathWashing\n",
    "\n",
    " [What is Math washing?](http://www.mathwashing.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machine Bias\n",
    "\n",
    "[Machine Bias — ProPublica](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kinds of Bias\n",
    "\n",
    "[Bias in Computer Systems - Friedman & Nissenbaum (1996)](https://nissenbaum.tech.cornell.edu/papers/Bias%20in%20Computer%20Systems.pdf)\n",
    "\n",
    "1. Pre-existing Bias\n",
    "    * Individual\n",
    "    * Societal\n",
    "2. Technical Bias\n",
    "    * Computer tools\n",
    "    * Decontextualized Algorithms\n",
    "    * Random number generation\n",
    "    * Formalization of Human Constructs\n",
    "3. Emergent Bias\n",
    "    * New Societal Knowledge\n",
    "    * Mismatch between users and system design\n",
    "        * Different expertise\n",
    "        * Different values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Assumptions\n",
    "\n",
    "[Challenging Dangerous Assumptions by Andy Cohen](https://youtu.be/So89DJ1Rckc)\n",
    "\n",
    "[4 Customer Segmentation Assumptions to Avoid  - D&B Canada](http://www.dnb.com/ca-en/perspectives/finance-credit-risk/four-key-assumptions-avoid-with-customer-segmentation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Algorithmic responsibility\n",
    "\n",
    "[Algorithmic Accountability: A Primer](https://apo.org.au/sites/default/files/resource-files/2018-04/apo-nid142131.pdf)\n",
    "\n",
    "1. Fairness and Bias\n",
    "2. Opacity and Transparency\n",
    "3. Repurposing Data and Repurposing Algorithms\n",
    "4. Lack of Standards for Auditing\n",
    "5. Power and Control\n",
    "6. Trust and Expertise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What kind of action?\n",
    "\n",
    "* Awareness of own perspectives -  [20 lessons on bias in machine learning systems from NIPS 2017 Keynote](https://hub.packtpub.com/20-lessons-bias-machine-learning-systems-nips-2017/)\n",
    "* Laws and regulation about data - [How GDPR Drives Real-Time Analytics](https://datafloq.com/read/gdpr-drives-real-time-analytics/4824?mkt_tok=eyJpIjoiWldNeVlURXlNV0kxTkRJeCIsInQiOiI0bWZVSzJvYlQzNGZkaXQ3Mzc1S1RsTFFyR0E1bHpHbG1DR3dqMHhNUlVcL01VdTV3dW03Q3VaZHRkcG9pZWlUbUlQMVltMFozR0E5N2VKZHduK3pQOWl3UFJ6UFgyUEFjaXJtbldXMGVZclhHZTBxS3BWQ2hrM281M29aRnRXRUoifQ%3D%3D)\n",
    "* Understanding segmenting and marginalised groups -  [Google Developers Blog: Text Embedding Models Contain Bias. Here’s Why That Matters.](https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html?m=1)\n",
    "* Consideration of user privacy\n",
    "* Building trust - [Looking beyond accuracy to improve trust in machine learning - codecentric AG Blog](https://blog.codecentric.de/en/2018/01/look-beyond-accuracy-improve-trust-machine-learning/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Social Dilemma\n",
    "\n",
    "[The Social Dilemma Of Driverless Cars | Iyad Rahwan | TEDxCambridge](https://youtu.be/nhCh1pBsS80)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
