{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "# Tutorial :: Threats and opportunities in external data - the power of the news"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "**CONCERN**\n",
                "\n",
                "You are working for a consultancy firm in charge of the Australian government's political image. In September 2021, the Australian government had a high-profile problem with France due to a deal to buy french submarines being called off. A report has already been generated with the titles of news items. However, your job as an analyst is to create a more thorough report taking into consideration additional information inside each news item.\n",
                "\n",
                "In particular, your clients want to be aware of **threats** and **opportunities** suggested by the news.\n",
                "\n",
                "1. **Q**uestion\n",
                "2. **D**ata\n",
                "3. **A**nalysis\n",
                "4. **V**isualisation\n",
                "5. **I**nsight\n",
                "\n",
                "<img src=\"graphics/QDAVI_cycle_sm.png\" width=\"50%\" />"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "### 1. Question\n",
                "\n",
                "How has the news affected the image of the Australian government?\n",
                "\n",
                "**Tip:** You can combine web scraping and APIs"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "slideshow": {
                    "slide_type": "slide"
                }
            },
            "source": [
                "### 2. Data\n",
                "\n",
                "You must use The Guardian API\n",
                "\n",
                "**Tip:** Check the studio session and tutorial notebooks from Week 3 for information about how to call the guardian API"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Libraries for the analysis\n",
                "import pandas as pd\n",
                "import requests\n",
                "import json\n",
                "from bs4 import BeautifulSoup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Build a search URL\n",
                "baseUrl = 'https://content.guardianapis.com/search?q=' # content search\n",
                "\n",
                "searchString = \"submarine\"\n",
                "office = \"&production-office=aus\"\n",
                "tag = \"&tag=politics/politics\"\n",
                "fromDate = \"&from-date=2021-09-01\"\n",
                "toDate = \"&to-date=2021-11-30\"\n",
                "\n",
                "url = baseUrl+'\"'+searchString+'\"'+office+fromDate+toDate+\"&api-key=test\"\n",
                "print(url)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Call the API\n",
                "response = requests.get(url)\n",
                "data = json.loads(response.content)\n",
                "results = data['response']['results']\n",
                "results"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The results contain the URL to the news items on the website. After inspecting a couple of pages, which information could be easily extracted from it"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Get HTML function\n",
                "def get_HTML(url):\n",
                "    # get data from server\n",
                "    response = requests.get(url)\n",
                "    html = response.content\n",
                "    return html"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Beautiful soup function for subtitle\n",
                "def extract_subTitle(HTML):\n",
                "    soup = BeautifulSoup(HTML, \"html.parser\") # the html input and the parser name\n",
                "    article = soup.find(\"article\") # the tag that contains the article\n",
                "    div_element = article.find(\"div\", attrs={\"data-gu-name\": \"standfirst\"}) # the tag that can be found using an attribute\n",
                "    if div_element is not None:\n",
                "        target_element = div_element.find(\"p\")\n",
                "        return target_element.text\n",
                "    else:\n",
                "        return \"\"\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Beautiful soup function for body\n",
                "def extract_body(HTML):\n",
                "    soup = BeautifulSoup(HTML, \"html.parser\") # the html input and the parser name\n",
                "    article = soup.find(\"article\") # the tag that contains the article\n",
                "    div_element = article.find(\"div\", attrs={\"id\": \"maincontent\"}) # the tag that can be found using an attribute\n",
                "    if div_element is not None:\n",
                "        div_div_element = div_element.find(\"div\")\n",
                "        target_elements = div_element.findAll(\"p\")\n",
                "        result = \"\"\n",
                "        for te in target_elements:\n",
                "            result += te.text\n",
                "        return result\n",
                "    else:\n",
                "        return \"\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Clean/preprocess data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Create a dataframe\n",
                "df = pd.DataFrame(columns=[\"Date\", \"Section\", \"Title\", \"Subtitle\", \"Body\"])\n",
                "df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Populate the dataframe\n",
                "for news in results:\n",
                "    html = get_HTML(news[\"webUrl\"])\n",
                "    data = {\"Date\": news[\"webPublicationDate\"], \"Section\": news[\"sectionName\"], \"Title\": news[\"webTitle\"], \"Subtitle\": extract_subTitle(html), \"Body\": extract_body(html)}\n",
                "    df_to_append = pd.DataFrame([data])\n",
                "    df = pd.concat([df,df_to_append], ignore_index=True)\n",
                "df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. Analysis\n",
                "\n",
                "Information extraction?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Inspect the data\n",
                "\n",
                "Read a few articles at random to get a feel for what is important to analyse."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "tags": []
            },
            "source": [
                "#### One approach - a basic sentiment analysis that looks for positive and negative words in the text"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define lists of positive and negative words\n",
                "positive_words = [\"good\", \"positive\", \"excellent\", \"success\"] # add words you think are good indicators\n",
                "negative_words = [\"bad\", \"poor\", \"negative\", \"disappointing\"]\n",
                "\n",
                "\n",
                "# Function to calculate a basic sentiment score\n",
                "def analyze_sentiment(article):\n",
                "    positive_count = 0\n",
                "    negative_count = 0\n",
                "    \n",
                "    # Convert article to lowercase and split into words\n",
                "    words = article.lower().split()\n",
                "    \n",
                "    # Count occurrences of positive and negative words\n",
                "    for word in words:\n",
                "        if word in positive_words:\n",
                "            positive_count += 1\n",
                "        if word in negative_words:\n",
                "            negative_count += 1\n",
                "            \n",
                "    # Compute sentiment score\n",
                "    sentiment_score = positive_count - negative_count\n",
                "    return sentiment_score\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Analyze the articles\n",
                "\n",
                "# Create a list of article bodies from the df column\n",
                "article_body_texts = df[\"Body\"].tolist()\n",
                "\n",
                "# Loop through and run the analyze_sentiment function on each\n",
                "for article_text in article_body_texts:\n",
                "    \n",
                "    score = analyze_sentiment(article_text)\n",
                "\n",
                "    # Print sentiment score\n",
                "    print(f\"Sentiment Score: {score}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Combine the scores and do some analysis\n",
                "\n",
                "##### Tip: You could add them as a new column of your existing df\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "???"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4. Visualisation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "???"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5. Insights\n",
                "\n",
                "What might be some limitations of how you analysed the data?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Scrape some data from the web to include as background in your final report\n",
                "\n",
                "Use code similar to that in this week's studio session to scrape some data from the web relevant to the submarine issue.\n",
                "\n",
                "Suggestion: A list of current Australian submarines with their names and launch dates scraped from the web like the one at https://en.wikipedia.org/wiki/Collins-class_submarine#Submarines_in_class)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "celltoolbar": "Slideshow",
        "hide_input": false,
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        },
        "qut": {
            "creation_period": "2023_sem2",
            "nb_name": "B2-TOWS-threats_opportunities-tutorial",
            "unit_code": "IAB303"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
