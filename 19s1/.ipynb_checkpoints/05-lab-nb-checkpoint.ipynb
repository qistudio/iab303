{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>IAB303</b> - Data Analytics for Business Insight</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 05 Lab :: Scraping & XML\n",
    "\n",
    "Tasks:\n",
    "1. Understanding How Information Is Given In Web Pages\n",
    "2. Finding How Specific Information In Web Pages Is Encoded\n",
    "3. Using Python To Automate Information Retrieval From Web Pages\n",
    "4. A Business Intelligence Scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Understanding How Information Is Given In Web Pages\n",
    "\n",
    "One of the most common ways of serving information online is through web pages. Web pages are usually given in HTML, a programming language that represents documents as well-structured elements, with sub-elements branching out from containing elements.\n",
    "\n",
    "As they 'branch' out further and further, they form what is typically called a DOM (Document Object Model) 'tree'.\n",
    "\n",
    "A DOM tree has a 'head' element, and a 'body' element, with most of the relevant viewing content being stored in the 'body' e.g."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<html>\n",
    "    <head>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div>\n",
    "            <div class=\"sub-element\"></div>\n",
    "        </div>\n",
    "    </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second practical, we used HTML tags to help us style some text. However, in this case, we'll be understanding it a bit further.\n",
    "\n",
    "To begin, open a web browser. Depending on the browser you are using, follow instructions in the following link to open its developer tools:\n",
    "\n",
    "https://www.lifewire.com/web-browser-developer-tools-3988965"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From there, a tabbed sub-window will appear that displays information about the current web page you have opened. It can tell you a lot about the page, although all we are concerned with is the 'Elements' section, which shows the actual HTML DOM tree of the page itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Finding How Specific Information In Web Pages Is Encoded\n",
    "\n",
    "Now that we can load up a web page's HTML just from opening it in a browser, let's try something a bit more specific. \n",
    "\n",
    "The following web page is the 'Wikipedia' article for Australia:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Australia\n",
    "\n",
    "Open a new tab and go to this page. On the page, you'll see in the right sidebar that the capital of Australia is 'Canberra'.\n",
    "\n",
    "Simply right-click the text, and hit the 'Inspect'/'Inspect Element' option. This will load up the 'Developer Tools', which will not only open up the 'Elements' tab of the page, but will jump to the location of the element in which the information is stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Using Python To Automate Information Retrieval From Web Pages\n",
    "\n",
    "So far, we have been given a clear understanding about how a page renders content in HTML, as well as how to trace information from a web page back to the element it is contained in within the DOM tree.\n",
    "\n",
    "In this task, we introduce 'BeautifulSoup', a powerful library for Python that enables us to automate information retrieval from a web page:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Before we can use 'BeautifulSoup', we have to install its library into our Jupyter Notebook together with a parsing library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /opt/conda/lib/python3.6/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.6/site-packages (from bs4) (4.6.3)\n",
      "Collecting html5lib\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/62/bbd2be0e7943ec8504b517e62bab011b4946e1258842bc159e5dfde15b96/html5lib-1.0.1-py2.py3-none-any.whl (117kB)\n",
      "\u001b[K    100% |████████████████████████████████| 122kB 410kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: webencodings in /opt/conda/lib/python3.6/site-packages (from html5lib) (0.5.1)\n",
      "Requirement already satisfied: six>=1.9 in /opt/conda/lib/python3.6/site-packages (from html5lib) (1.12.0)\n",
      "Installing collected packages: html5lib\n",
      "Successfully installed html5lib-1.0.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install bs4\n",
    "#!pip install html5lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate that you now have the module installed, execute the following 'cell'. Provided that you have installed it correctly, the cell will run without error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import html5lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the library installed, its time to use it. BeautifulSoup can interpret the DOM tree from a HTML document, so that we can easily pull out elements from the page with simple expressions. Take for instance the following HTML that we load into a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_HTML_page = \\\n",
    "    '<html>'\\\n",
    "    '   <head>'\\\n",
    "    '   </head>'\\\n",
    "    '   <body>'\\\n",
    "    '      <div>Not Here</div>'\\\n",
    "    '      <div class=\"target\">The Text We Are After</div>'\\\n",
    "    '   </body>'\\\n",
    "    '</html>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using BeautifulSoup, we first interpret the page into a variable. From here, there are many possible ways of getting the target element (element with the 'class' of value 'target'). \n",
    "\n",
    "The most obvious way for this scenario involves finding the first element with the class of 'target':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"target\">The Text We Are After</div>\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(some_HTML_page, \"html.parser\")\n",
    "\n",
    "for element in soup(attrs={'class' : 'target'}):\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In more complex situations, we might not know the target element's class value, but may know details about its previous element (e.g. the text inside the previous element):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"target\">The Text We Are After</div>\n"
     ]
    }
   ],
   "source": [
    "element = soup.find(text=???) # the text before\n",
    "print(element.findNext(???))  # the tag that you want to find"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to run BeatifulSoup on an actual web page, we could simply call the 'requests' library to load down the raw text of that page. Here we specify a basic method for pulling down HTML from a real web page, by specifying its URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "def get_HTML(url):\n",
    "    response = urllib.request.urlopen(???)\n",
    "    html = response.read()\n",
    "    return ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalling the Wikipedia page for Australia, we can then get its raw HTML using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Australia_Wiki_HTML = get_HTML('https://en.wikipedia.org/wiki/Australia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next question is: What are details about the element in which the name of Australia's capital city is stored?\n",
    "\n",
    "ANSWER: From perusing the elements in the 'Developer Tools' window, we can state the following facts about our target element:\n",
    "\n",
    "* It has an 'a' tag\n",
    "* It is inside an element with a 'td' tag\n",
    "* The element with the 'td' tag is preceded by an element with a 'th' tag\n",
    "* The element with the 'th' tag contains the text 'Capital'\n",
    "\n",
    "So the code that would get the exact element we are after is described in the method below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"/wiki/Canberra\" title=\"Canberra\">Canberra</a>\n"
     ]
    }
   ],
   "source": [
    "def get_the_capital(HTML):\n",
    "    soup = BeautifulSoup(???, ???) # the html input and the parser name\n",
    "    th_element = soup.find(text=???) # the text that we are looking for\n",
    "    target_element = th_element.findNext(???) # the tag that we are looking for\n",
    "    print(target_element)\n",
    "\n",
    "get_the_capital(Australia_Wiki_HTML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before reading any further, follow this link (https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to learn more about the functions ('find', 'findNext') used in the code above.\n",
    "\n",
    "To demonstrate just how flexible our solution is, we can run the exact same method on a different country e.g. France:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"/wiki/Paris\" title=\"Paris\">Paris</a>\n"
     ]
    }
   ],
   "source": [
    "France_Wiki_HTML = get_HTML('https://en.wikipedia.org/wiki/France')\n",
    "get_the_capital(France_Wiki_HTML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - A Business Intelligence Scenario\n",
    "\n",
    "As a market analyst working for a tourism agency, your boss has approached you with a client in need of a recommendation regarding the top tourist destinations of 2018.\n",
    "\n",
    "While this may sound easy, in hopes that it will improve their tourism experience, the client has also requested that places that are more innovative be prioritised in the recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately for this task, the top tourist destinations of 2018 are stored on the following URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tourism_destinations = 'https://en.wikipedia.org/wiki/World_Tourism_rankings'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Developer Tools, identify things that could be used to isolate the names of the countries in the table, in the section entitled \"Most visited destinations by international tourist arrivals\". \n",
    "\n",
    "For this task, the details have been given, however, the code that retrieves the values is only half completed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details:\n",
    "     * A 'span' element contains a 'h2' element with the title of the target 'table' inside it.\n",
    "     * A 'table' element proceeds the 'span' element.\n",
    "     * There are 'td' elements inside the 'table' element.\n",
    "     * Each 'td' element has an attribute of 'align' with the value 'left'.\n",
    "     * In each 'td' element, there is an 'a' element with the name of a given country inside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['France',\n",
       " 'Spain',\n",
       " 'United States',\n",
       " 'China',\n",
       " 'Italy',\n",
       " 'Mexico',\n",
       " 'United Kingdom',\n",
       " 'Turkey',\n",
       " 'Germany',\n",
       " 'Thailand']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_tourist_locations = []\n",
    "\n",
    "Tourism_Wiki_HTML = get_HTML(???)\n",
    "soup = BeautifulSoup(???, \"html.parser\")\n",
    "span_element = soup.find(text=???)\n",
    "h2_element = span_element.parent\n",
    "table_element = h2_element.findNext(???) # a parent tag\n",
    "for td_element in table_element.findAll(???,attrs={'align':'left'}): # a tag with specific attributes\n",
    "    a_element = td_element.find(???) # the tag we are looking for\n",
    "    if a_element != None:\n",
    "        top_tourist_locations.append(a_element.text)\n",
    "\n",
    "# If you enter the missing code, this will return a list of names of the top tourist destinations for 2018.\n",
    "top_tourist_locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing that the client is also looking for places that have higher innovation, what could we use from a single country's Wikipedia page to determine this quality?\n",
    "\n",
    "Going back to 'https://en.wikipedia.org/wiki/Australia', the HDI of the country will give a good indication of this; so how do we describe the HDI?\n",
    "\n",
    "Once again, here are some details to help:\n",
    "\n",
    "   * The text 'HDI' is in an 'a' element.\n",
    "   * The 'a' element is in a 'th' element.\n",
    "   * The 'th' is proceeded by a 'td' element.\n",
    "   * The 'td' element contains an 'img' element.\n",
    "   * Next to the 'img' element is the HDI value.\n",
    "\n",
    "The code that retrieves the HDI from a country's Wikipedia page is included in the following method, but it is incomplete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.901'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_country_HDI(html):\n",
    "    soup = BeautifulSoup(html, ???)\n",
    "    a_element = soup.find('a',text=???)\n",
    "    th_element = a_element.parent\n",
    "    td_element = th_element.findNext(???)\n",
    "    HDI_value = td_element.find(???).findNextSibling(text=True)\n",
    "    return HDI_value.strip()\n",
    "\n",
    "# If you enter the missing code, this function will produce the value '0.897'\n",
    "get_country_HDI(France_Wiki_HTML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all we have to do to get the HDI of each country is to substitute each country's name into the Wikipedia country's URL, and to feed the returned HTML into the 'get_country_HDI' method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: France\n",
      "Ranking: 1\n",
      "HDI: 0.901\n",
      "\n",
      "\n",
      "Country: Spain\n",
      "Ranking: 2\n",
      "HDI: 0.891\n",
      "\n",
      "\n",
      "Country: United States\n",
      "Ranking: 3\n",
      "HDI: 0.924\n",
      "\n",
      "\n",
      "Country: China\n",
      "Ranking: 4\n",
      "HDI: 0.752\n",
      "\n",
      "\n",
      "Country: Italy\n",
      "Ranking: 5\n",
      "HDI: 0.880\n",
      "\n",
      "\n",
      "Country: Mexico\n",
      "Ranking: 6\n",
      "HDI: 0.774\n",
      "\n",
      "\n",
      "Country: United Kingdom\n",
      "Ranking: 7\n",
      "HDI: 0.922\n",
      "\n",
      "\n",
      "Country: Turkey\n",
      "Ranking: 8\n",
      "HDI: 0.791\n",
      "\n",
      "\n",
      "Country: Germany\n",
      "Ranking: 9\n",
      "HDI: 0.936\n",
      "\n",
      "\n",
      "Country: Thailand\n",
      "Ranking: 10\n",
      "HDI: 0.755\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(top_tourist_locations)):\n",
    "    print(\"Country: \"+top_tourist_locations[i])\n",
    "    print(\"Ranking: \"+str(i+1))\n",
    "    print(\"HDI: \"+get_country_HDI(\n",
    "        get_HTML('https://en.wikipedia.org/wiki/'+top_tourist_locations[i].replace(' ','%20'))\n",
    "    ))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing rankings and HDIs, what would you state in your recommendation:"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
